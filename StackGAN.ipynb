{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LATEST of StackGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uscBeePlA3Se"
      },
      "source": [
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip install tensorflow-gpu==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROkivRBLN6pM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "cbe2d47f-70c2-406a-e60c-e0d5e107e426"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ngLlWRxqAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e8dab6d7-9583-4e11-b477-21303b945314"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2vJ-YBL9U9"
      },
      "source": [
        "#Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajlV6kDMIYf8"
      },
      "source": [
        "WORKERS = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbc9tETlahi-"
      },
      "source": [
        "class Dataset():\n",
        "  def __init__(self, image_size, batch_size = 64):\n",
        "    data_dir = \"/content/drive/My Drive/Text to Image/dataset/birds\"\n",
        "    train_dir = data_dir + \"/train\"\n",
        "    test_dir = data_dir + \"/test\"\n",
        "    train_embeddings_path = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    test_embeddings_path = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "    train_filenames_path = train_dir + \"/filenames.pickle\"\n",
        "    test_filenames_path = test_dir + \"/filenames.pickle\"\n",
        "    cub_dataset_dir = \"/content/drive/My Drive/Text to Image/dataset/CUB_200_2011/CUB_200_2011\"\n",
        "    bounding_boxes_path = cub_dataset_dir + \"/bounding_boxes.txt\"\n",
        "    image_ids_path = cub_dataset_dir + \"/images.txt\"\n",
        "    images_path = cub_dataset_dir + \"/images\"\n",
        "    \n",
        "    self.image_width = image_size[0]\n",
        "    self.image_height = image_size[1]\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    with open(train_filenames_path, 'rb') as f:\n",
        "      self.train_filenames = pickle.load(f, encoding='latin1')\n",
        "      self.train_filenames = [os.path.join(images_path, filename) + '.jpg' for filename in self.train_filenames]\n",
        "    \n",
        "    with open(test_filenames_path, 'rb') as f:\n",
        "      self.test_filenames = pickle.load(f, encoding='latin1')\n",
        "      self.test_filenames = [os.path.join(images_path, filename) + '.jpg' for filename in self.test_filenames]\n",
        "\n",
        "    with open(train_embeddings_path, 'rb') as f:\n",
        "      self.train_embeddings = pickle.load(f, encoding = 'latin1')\n",
        "\n",
        "    with open(test_embeddings_path, 'rb') as f:\n",
        "      self.test_embeddings = pickle.load(f, encoding = 'latin1')\n",
        "\n",
        "    bounding_boxes = {}\n",
        "    with open(bounding_boxes_path, 'rb') as f:\n",
        "      box_coordinates = f.read()\n",
        "      box_coordinates = box_coordinates.splitlines()\n",
        "      box_coordinates = [box_coordinate.decode('utf-8') for box_coordinate in box_coordinates]\n",
        "      for i in range(len(box_coordinates)):\n",
        "        bounding_box = box_coordinates[i].split()\n",
        "        bounding_boxes[bounding_box[0]] = [int(float(c)) for c in box_coordinates[i].split()][1:]\n",
        "\n",
        "    image_ids_mapping = {}\n",
        "    with open(image_ids_path, 'rb') as f:\n",
        "      image_ids = f.read()\n",
        "      image_ids = image_ids.splitlines()\n",
        "      image_ids = [image_id.decode('utf-8') for image_id in image_ids]\n",
        "      for i in range(len(image_ids)):\n",
        "        image_id = image_ids[i].split()\n",
        "        image_ids_mapping[image_id[0]] = image_id[1]\n",
        "\n",
        "    bounding_boxes_mapping = {}\n",
        "    for image_id in bounding_boxes.keys():\n",
        "      bounding_boxes_mapping[images_path + \"/\" + image_ids_mapping[image_id]] = bounding_boxes[image_id]\n",
        "\n",
        "    self.train_bounding_boxes = []\n",
        "    self.test_bounding_boxes = []\n",
        "    for i in range(len(self.train_filenames)):\n",
        "      self.train_bounding_boxes.append(bounding_boxes_mapping[self.train_filenames[i]])\n",
        "    for i in range(len(self.test_filenames)):\n",
        "      self.test_bounding_boxes.append(bounding_boxes_mapping[self.test_filenames[i]])\n",
        "\n",
        "  def crop(self, image, bounding_box):\n",
        "    image = image.numpy()\n",
        "    if bounding_box is not None:\n",
        "      x, y, width, height = bounding_box\n",
        "      image = image[y:(y + height), x:(x + width)]\n",
        "      image = cv2.resize(image, (self.image_width, self.image_height))\n",
        "    return image\n",
        "\n",
        "  def parse_function(self, image_path, embeddings, bounding_box):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels = 3)\n",
        "    # image = tf.image.crop_to_bounding_box(image, bounding_box[1], bounding_box[0], bounding_box[3], bounding_box[2])\n",
        "    image = tf.py_function(func = self.crop, inp = [image, bounding_box], Tout = tf.float32)\n",
        "    image.set_shape([self.image_width, self.image_height, 3])\n",
        "    image = (image - 127.5) / 127.5\n",
        "    # image = tf.image.resize(image, [self.image_width, self.image_height])\n",
        "\n",
        "    embedding_index = np.random.randint(0, embeddings.shape[0] - 1)\n",
        "    embedding = embeddings[embedding_index]\n",
        "    return image, embedding\n",
        "  \n",
        "  def get_train_ds(self):\n",
        "    BUFFER_SIZE = len(self.train_filenames)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((self.train_filenames, self.train_embeddings, self.train_bounding_boxes))\n",
        "    ds = ds.shuffle(BUFFER_SIZE)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.map(self.parse_function, num_parallel_calls = WORKERS)\n",
        "    ds = ds.batch(self.batch_size, drop_remainder = True)\n",
        "    ds = ds.prefetch(5)\n",
        "    return ds\n",
        "  \n",
        "  def get_test_ds(self):\n",
        "    BUFFER_SIZE = len(self.test_filenames)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((self.test_filenames, self.test_embeddings, self.test_bounding_boxes))\n",
        "    ds = ds.shuffle(BUFFER_SIZE)\n",
        "    ds = ds.repeat(1)\n",
        "    ds = ds.map(self.parse_function, num_parallel_calls = WORKERS)\n",
        "    ds = ds.batch(self.batch_size, drop_remainder = True)\n",
        "    ds = ds.prefetch(1)\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLPBCNUeLygV"
      },
      "source": [
        "#Stage 1 StackGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgs8mTisIN7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "dd4cb4b5-865e-4b51-b141-12d4d79e9d41"
      },
      "source": [
        "\"\"\"class generate_condition(layers.layer):\n",
        "  def __init__(self):\n",
        "    super(generate_condition, self).__init__()\n",
        "    self.dense = tf.keras.layers.Dense(units = 256)\n",
        "\n",
        "\n",
        "  def call(self, c_var):\n",
        "    X=tf.keras.layers.Flatten(c_var)\n",
        "    X = self.dense(X)\n",
        "    phi = tf.nn.leaky_relu(X)\n",
        "    mean = phi[:, :128]\n",
        "    std = phi[:, 128:]\n",
        "    epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n",
        "    C = mean + epsilon*std\n",
        "    return mean, std\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class generate_condition(layers.layer):\\n  def __init__(self):\\n    super(generate_condition, self).__init__()\\n    self.dense = tf.keras.layers.Dense(units = 256)\\n\\n\\n  def call(self, c_var):\\n    X=tf.keras.layers.Flatten(c_var)\\n    X = self.dense(X)\\n    phi = tf.nn.leaky_relu(X)\\n    mean = phi[:, :128]\\n    std = phi[:, 128:]\\n    epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\\n    C = mean + epsilon*std\\n    return mean, std\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ0ATRc6IN35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "e7216cd0-a99a-49ac-e786-7d5759064f6d"
      },
      "source": [
        "\n",
        "\"\"\"class sample_encoded_context(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ConditioningAugmentation, self).__init__()\n",
        "    self.dense = tf.keras.layers.Dense(units = 256)\n",
        "\n",
        "  def call(self, E):\n",
        "    X = self.dense(E)\n",
        "    phi = tf.nn.leaky_relu(X)\n",
        "    mean = phi[:, :128]\n",
        "    std = K.exp(phi[:, 128:])\n",
        "    epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n",
        "    C = mean + epsilon*std\n",
        "    return C, phi\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class sample_encoded_context(tf.keras.Model):\\n  def __init__(self):\\n    super(ConditioningAugmentation, self).__init__()\\n    self.dense = tf.keras.layers.Dense(units = 256)\\n\\n  def call(self, E):\\n    X = self.dense(E)\\n    phi = tf.nn.leaky_relu(X)\\n    mean = phi[:, :128]\\n    std = K.exp(phi[:, 128:])\\n    epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\\n    C = mean + epsilon*std\\n    return C, phi\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpWNw-pxIN0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "7f2ddec8-c2cf-482a-aa53-ecbe5d18bc6f"
      },
      "source": [
        "\"\"\"def sample_encoded_context(self, embeddings):\n",
        "        '''Helper function for init_opt'''\n",
        "        c_mean_logsigma = self.model.generate_condition(embeddings)\n",
        "        mean = c_mean_logsigma[0]\n",
        "        if cfg.TRAIN.COND_AUGMENTATION:\n",
        "            # epsilon = tf.random_normal(tf.shape(mean))\n",
        "            epsilon = tf.truncated_normal(tf.shape(mean))\n",
        "            stddev = tf.exp(c_mean_logsigma[1])\n",
        "            c = mean + stddev * epsilon\n",
        "\n",
        "            kl_loss = KL_loss(c_mean_logsigma[0], c_mean_logsigma[1])\n",
        "        else:\n",
        "            c = mean\n",
        "            kl_loss = 0\n",
        "\n",
        "        return c, cfg.TRAIN.COEFF.KL * kl_loss\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def sample_encoded_context(self, embeddings):\\n        '''Helper function for init_opt'''\\n        c_mean_logsigma = self.model.generate_condition(embeddings)\\n        mean = c_mean_logsigma[0]\\n        if cfg.TRAIN.COND_AUGMENTATION:\\n            # epsilon = tf.random_normal(tf.shape(mean))\\n            epsilon = tf.truncated_normal(tf.shape(mean))\\n            stddev = tf.exp(c_mean_logsigma[1])\\n            c = mean + stddev * epsilon\\n\\n            kl_loss = KL_loss(c_mean_logsigma[0], c_mean_logsigma[1])\\n        else:\\n            c = mean\\n            kl_loss = 0\\n\\n        return c, cfg.TRAIN.COEFF.KL * kl_loss\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2aAthY3INxE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkrXDsWPwTF4"
      },
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "  mean = y_pred[:, :128]\n",
        "  logsigma = y_pred[:, 128:]\n",
        "  loss = -logsigma + 0.5*(-1 + K.exp(2.0*logsigma) + K.square(mean))\n",
        "  loss = K.mean(loss)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRcUy6S1Ckx5"
      },
      "source": [
        "class ConditioningAugmentation(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ConditioningAugmentation, self).__init__()\n",
        "    self.dense = tf.keras.layers.Dense(units = 256)\n",
        "\n",
        "  def call(self, E):\n",
        "    X=tf.keras.layers.Flatten()(E)\n",
        "    X = self.dense(X)\n",
        "    phi = tf.nn.leaky_relu(X)\n",
        "    mean = phi[:, :128]\n",
        "    std = K.exp(phi[:, 128:])\n",
        "    #epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n",
        "    epsilon=tf.keras.backend.truncated_normal(shape=(mean.shape))\n",
        "    C = mean + epsilon*std\n",
        "    return C, phi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhVBzTUnDMmT"
      },
      "source": [
        "class EmbeddingCompressor(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(EmbeddingCompressor, self).__init__()\n",
        "    self.dense = tf.keras.layers.Dense(units = 128)\n",
        "\n",
        "  def call(self, E):\n",
        "    X = self.dense(E)\n",
        "    return tf.nn.relu(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cn9qNR721K-"
      },
      "source": [
        "class Stage1Generator(tf.keras.Model):\n",
        "  def __init__(self,image_shape=[64,64]):\n",
        "    super(Stage1Generator, self).__init__()\n",
        "    self.s = image_shape[0]\n",
        "    self.s2, self.s4, self.s8, self.s16 = int(self.s / 2), int(self.s / 4), int(self.s / 8), int(self.s / 16)\n",
        "    self.canet = ConditioningAugmentation()\n",
        "    self.concat = tf.keras.layers.Concatenate(axis = 1)\n",
        "    self.dense = tf.keras.layers.Dense(units =128*4*4*8 , kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n",
        "    self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*8), input_shape = (128*8*4*4, ))\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.9)\n",
        "    \n",
        "    #self.resblock1= Resblock([256,256,1024],[1,3,3])\n",
        "    self.conv5 = tf.keras.layers.Conv2D(256,1,strides=1,padding=\"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm5= tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv6= tf.keras.layers.Conv2D(256,3,strides=1,padding=\"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm6 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "\n",
        "    self.conv7= tf.keras.layers.Conv2D(1024,3,strides=1,padding=\"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm7 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "\n",
        "    self.conv8=tf.keras.layers.Conv2D(512,3,1,padding='same', kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm8 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "\n",
        "    #self.resblock2=Resblock([256,256,512],[1,3,3])\n",
        "    self.conv9 = tf.keras.layers.Conv2D(128,1,strides=1,padding=\"same\",kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm9= tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv10= tf.keras.layers.Conv2D(128,3,strides=1,padding=\"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm10 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv11= tf.keras.layers.Conv2D(512,3,strides=1,padding=\"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm11 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "  \n",
        "    self.conv3=tf.keras.layers.Conv2D(256,3,1,padding='same', kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm3 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv4=tf.keras.layers.Conv2D(128,3,1,padding='same', kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm4 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "  \n",
        "    self.conv12=tf.keras.layers.Conv2D(3,3,1,padding='same', kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    \n",
        "\n",
        "  def call(self, inputs,istraining=True):\n",
        "    E, Z = inputs\n",
        "    C, phi = self.canet(E)\n",
        "\n",
        "    gen_input = self.concat([C, Z])\n",
        "    gen_input = tf.keras.layers.Flatten()(gen_input)\n",
        "    X = self.dense(gen_input)\n",
        "    X = self.batchnorm1(X) ##\n",
        "    R = self.reshape(X) ##\n",
        "    ## X = self.batchnorm1(X)\n",
        "    ## R = tf.nn.relu(X)\n",
        "    \n",
        "    \n",
        "    #X = tf.nn.relu(X\n",
        "\n",
        "    #X = self.resblock1(X)\n",
        "    X=self.conv5(R)\n",
        "    X=self.batchnorm5(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X=self.conv6(X)\n",
        "    X=self.batchnorm6(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X=self.conv7(X)\n",
        "    X=self.batchnorm7(X,training=istraining)\n",
        "\n",
        "    X = tf.math.add(R,X)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X = tf.compat.v1.image.resize_nearest_neighbor(X, [self.s8, self.s8], align_corners=False, name=None, half_pixel_centers=False)\n",
        "    X= self.conv8(X)\n",
        "    R2= self.batchnorm8(X,training=istraining)\n",
        "\n",
        "    #X= self.resblock2(X)\n",
        "    X=self.conv9(R2)\n",
        "    X=self.batchnorm9(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X=self.conv10(X)\n",
        "    X=self.batchnorm10(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X=self.conv11(X)\n",
        "    X=self.batchnorm11(X,training=istraining)\n",
        "    \n",
        "    X = tf.math.add(X,R2)\n",
        "    X=tf.nn.relu(X)\n",
        "\n",
        "    X = tf.compat.v1.image.resize_nearest_neighbor(X, [self.s4, self.s4], align_corners=False, name=None, half_pixel_centers=False)\n",
        "    X = self.conv3(X)\n",
        "    X= self.batchnorm3(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "    X = tf.compat.v1.image.resize_nearest_neighbor(X, [self.s2, self.s2], align_corners=False, name=None, half_pixel_centers=False)\n",
        "    X = self.conv4(X)\n",
        "    X= self.batchnorm4(X,training=istraining)\n",
        "    X=tf.nn.relu(X)\n",
        "    X = tf.compat.v1.image.resize_nearest_neighbor(X, [self.s, self.s], align_corners=False, name=None, half_pixel_centers=False)\n",
        "    X = self.conv12(X)\n",
        "\n",
        "\n",
        "    \n",
        "    return tf.nn.tanh(X), phi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F89MagHt4BHc"
      },
      "source": [
        "class Stage1Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage1Discriminator, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm2 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm3 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv5 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm4 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm5 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv7 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm6 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.embed = EmbeddingCompressor()\n",
        "    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n",
        "    self.concat = tf.keras.layers.Concatenate()\n",
        "    self.conv8 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm7 = tf.keras.layers.BatchNormalization(gamma_initializer=tf.keras.initializers.RandomNormal(1.0,0.02),axis = -1, momentum = 0.9)\n",
        "    self.conv9 = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 4, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    I, E = inputs\n",
        "    X = self.conv1(I)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    X = self.conv2(X)\n",
        "    X = self.batchnorm1(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    X = self.conv3(X)\n",
        "    X = self.batchnorm2(X)\n",
        "\n",
        "\n",
        "    X = self.conv4(X)\n",
        "    R = self.batchnorm3(X)\n",
        "    \n",
        "\n",
        "    X = self.conv5(R)\n",
        "    X = self.batchnorm4(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    X = self.conv6(X)\n",
        "    X = self.batchnorm5(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    X = self.conv7(X)\n",
        "    X = self.batchnorm6(X)\n",
        "    X = tf.math.add(X,R)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    T = self.embed(E)\n",
        "    T = self.reshape(T)\n",
        "    T = tf.tile(T, (1, 4, 4, 1))\n",
        "    merged_input = self.concat([X, T])\n",
        "\n",
        "    Y = self.conv8(merged_input)\n",
        "    Y = self.batchnorm7(Y)\n",
        "    Y = tf.nn.leaky_relu(Y)\n",
        "\n",
        "    Y = self.conv9(Y)\n",
        "    return tf.squeeze(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxxCAPXgUUgl"
      },
      "source": [
        "class Stage1Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage1Model, self).__init__()\n",
        "    self.stage1_generator = Stage1Generator()\n",
        "    self.stage1_discriminator = Stage1Discriminator()\n",
        "\n",
        "  def train(self, train_ds, batch_size = 64, num_epochs = 600, z_dim = 100, c_dim = 128, stage1_generator_lr = 0.0002, stage1_discriminator_lr = 0.0002):\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"Epoch %d/%d:\\n [\"%(epoch + 1, num_epochs), end = \"\")\n",
        "      start_time = time.time()\n",
        "      if epoch % 50 == 0:\n",
        "        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n",
        "        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n",
        "    \n",
        "      generator_loss_log = []\n",
        "      discriminator_loss_log = []\n",
        "      steps_per_epoch = 125\n",
        "      batch_iter = iter(train_ds)\n",
        "      for i in range(steps_per_epoch):\n",
        "        if i % 5 == 0:\n",
        "          print(\"=\", end = \"\")\n",
        "        image_batch, embedding_batch = next(batch_iter)\n",
        "        z_noise = tf.random.normal((batch_size, z_dim))\n",
        "\n",
        "        mismatched_images = tf.roll(image_batch, shift = 1, axis = 0)\n",
        "\n",
        "        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n",
        "        fake_labels = tf.zeros(shape = (batch_size, ))\n",
        "        mismatched_labels = tf.zeros(shape = (batch_size, ))\n",
        "        disc_labels=tf.concat([real_labels,fake_labels],axis=0)\n",
        "\n",
        "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "          fake_images, phi = self.stage1_generator([embedding_batch, z_noise])\n",
        "          \n",
        "          \n",
        "          real_logits = self.stage1_discriminator([image_batch, embedding_batch])\n",
        "          fake_logits =  self.stage1_discriminator([fake_images, embedding_batch])\n",
        "          mismatched_logits = self.stage1_discriminator([mismatched_images, embedding_batch])\n",
        "          disc_logits=tf.concat([real_logits,fake_logits],axis=0)\n",
        "         \n",
        "          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n",
        "          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "          generator_loss = l_sup + 2*l_klreg\n",
        "          \n",
        "          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
        "          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
        "          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
        "          discriminator_loss = tf.add(l_real,0.5*tf.add(l_mismatched,l_fake))\n",
        "        \n",
        "        generator_gradients = generator_tape.gradient(generator_loss, self.stage1_generator.trainable_variables)\n",
        "        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage1_discriminator.trainable_variables)\n",
        "        \n",
        "        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage1_generator.trainable_variables))\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage1_discriminator.trainable_variables))\n",
        "        \n",
        "        generator_loss_log.append(generator_loss)\n",
        "        discriminator_loss_log.append(discriminator_loss)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "        epoch_time = end_time - start_time\n",
        "        template = \"] - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "        print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "      if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "        save_path = \"/content/drive/My Drive/Text to Image/lr_results/epoch_\" + str(epoch + 1)\n",
        "        temp_embeddings = None\n",
        "        for _, embeddings in train_ds:\n",
        "          temp_embeddings = embeddings.numpy()\n",
        "          break\n",
        "        if os.path.exists(save_path) == False:\n",
        "          os.makedirs(save_path)\n",
        "        temp_batch_size = 10\n",
        "        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n",
        "        temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
        "        fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise],istraining=False)\n",
        "        for i, image in enumerate(fake_images):\n",
        "          image = 127.5*image + 127.5\n",
        "          image = image.numpy().astype('uint8')\n",
        "          cv2.imwrite(save_path + \"/gen_%d.png\"%(i), image)\n",
        "      \n",
        "        self.stage1_generator.save_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_generator_\" + str(epoch + 1) + \".ckpt\")\n",
        "        self.stage1_discriminator.save_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_discriminator_\" + str(epoch + 1) + \".ckpt\")\n",
        "\n",
        "    def generate_image(self, embedding):\n",
        "      self.stage1_generator.compile(loss = \"mse\", optimizer = \"adam\")\n",
        "      self.stage1_generator.load_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_generator_600.ckpt\").expect_partial()\n",
        "      z_noise = tf.random.normal((batch_size, z_dim))\n",
        "      generated_image = self.stage1_generator([embedding, z_noise])\n",
        "      return generated_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stxYnUo4TN86"
      },
      "source": [
        "dataset = Dataset(image_size = (256, 256),batch_size=16)\n",
        "train_ds = dataset.get_train_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbMjw3aG4AUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "040748f1-f512-494b-deb7-ee44b672e8b4"
      },
      "source": [
        "model = Stage1Model()\n",
        "model.train(train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600:\n",
            " [=========================] - generator_loss: 0.7553 - discriminator_loss: 1.3827 - epoch_time: 47.31 s\n",
            "Epoch 2/600:\n",
            " [=========================] - generator_loss: 0.7576 - discriminator_loss: 1.3787 - epoch_time: 50.21 s\n",
            "Epoch 3/600:\n",
            " [=========================] - generator_loss: 0.8260 - discriminator_loss: 1.3554 - epoch_time: 51.85 s\n",
            "Epoch 4/600:\n",
            " [=========================] - generator_loss: 0.7702 - discriminator_loss: 1.3738 - epoch_time: 53.48 s\n",
            "Epoch 5/600:\n",
            " [=========================] - generator_loss: 0.8534 - discriminator_loss: 1.3456 - epoch_time: 52.69 s\n",
            "Epoch 6/600:\n",
            " [=========================] - generator_loss: 0.8513 - discriminator_loss: 1.3476 - epoch_time: 52.75 s\n",
            "Epoch 7/600:\n",
            " [=========================] - generator_loss: 0.8720 - discriminator_loss: 1.3339 - epoch_time: 49.13 s\n",
            "Epoch 8/600:\n",
            " [=========================] - generator_loss: 0.9012 - discriminator_loss: 1.3109 - epoch_time: 53.52 s\n",
            "Epoch 9/600:\n",
            " [=========================] - generator_loss: 1.0055 - discriminator_loss: 1.2679 - epoch_time: 54.33 s\n",
            "Epoch 10/600:\n",
            " [=========================] - generator_loss: 0.9015 - discriminator_loss: 1.3091 - epoch_time: 47.06 s\n",
            "Epoch 11/600:\n",
            " [=========================] - generator_loss: 0.8366 - discriminator_loss: 1.3331 - epoch_time: 42.82 s\n",
            "Epoch 12/600:\n",
            " [=========================] - generator_loss: 0.8680 - discriminator_loss: 1.3062 - epoch_time: 43.12 s\n",
            "Epoch 13/600:\n",
            " [=========================] - generator_loss: 0.8853 - discriminator_loss: 1.3036 - epoch_time: 42.56 s\n",
            "Epoch 14/600:\n",
            " [=========================] - generator_loss: 0.9039 - discriminator_loss: 1.2869 - epoch_time: 42.80 s\n",
            "Epoch 15/600:\n",
            " [=========================] - generator_loss: 0.9804 - discriminator_loss: 1.2639 - epoch_time: 42.54 s\n",
            "Epoch 16/600:\n",
            " [=========================] - generator_loss: 0.9219 - discriminator_loss: 1.2768 - epoch_time: 42.95 s\n",
            "Epoch 17/600:\n",
            " [=========================] - generator_loss: 0.9617 - discriminator_loss: 1.2506 - epoch_time: 42.75 s\n",
            "Epoch 18/600:\n",
            " [=========================] - generator_loss: 0.9414 - discriminator_loss: 1.2612 - epoch_time: 43.80 s\n",
            "Epoch 19/600:\n",
            " [=========================] - generator_loss: 0.9424 - discriminator_loss: 1.2565 - epoch_time: 43.36 s\n",
            "Epoch 20/600:\n",
            " [=========================] - generator_loss: 0.9470 - discriminator_loss: 1.2560 - epoch_time: 43.56 s\n",
            "Epoch 21/600:\n",
            " [=========================] - generator_loss: 0.9330 - discriminator_loss: 1.2530 - epoch_time: 43.24 s\n",
            "Epoch 22/600:\n",
            " [=========================] - generator_loss: 0.9688 - discriminator_loss: 1.2401 - epoch_time: 42.80 s\n",
            "Epoch 23/600:\n",
            " [=========================] - generator_loss: 0.9048 - discriminator_loss: 1.2598 - epoch_time: 43.20 s\n",
            "Epoch 24/600:\n",
            " [==============="
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtvKCODqLitp"
      },
      "source": [
        "#Stage 2 StackGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzzZriSwwyx"
      },
      "source": [
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters = 128*4, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters = 128*4, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "\n",
        "    def call(self, I):\n",
        "      X = self.conv1(I)\n",
        "      X = self.batchnorm1(X)\n",
        "      X = tf.nn.relu(X)\n",
        "\n",
        "      X = self.conv2(X)\n",
        "      X = self.batchnorm2(X)\n",
        "      X = tf.nn.relu(X)\n",
        "      X = tf.keras.layers.Add()([X, I])\n",
        "      X = tf.nn.relu(X)\n",
        "      return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGeTGuO-SGeG"
      },
      "source": [
        "\n",
        "class Stage2Generator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage2Generator, self).__init__()\n",
        "    self.canet = ConditioningAugmentation()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.conv2 = tf.keras.layers.Conv2D(256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv3 = tf.keras.layers.Conv2D(512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv4 = tf.keras.layers.Conv2D(512, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.resblock1 = ResidualBlock()\n",
        "    self.resblock2 = ResidualBlock()\n",
        "    self.resblock3 = ResidualBlock()\n",
        "    self.resblock4 = ResidualBlock()\n",
        "    self.upsamp1 = tf.keras.layers.UpSampling2D(size = (2, 2))\n",
        "    self.conv5 = tf.keras.layers.Conv2D(256, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.upsamp2 = tf.keras.layers.UpSampling2D(size = (2, 2))\n",
        "    self.conv6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm5 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.upsamp3 = tf.keras.layers.UpSampling2D(size = (2, 2))\n",
        "    self.conv7 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm6 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.upsamp4 = tf.keras.layers.UpSampling2D(size = (2, 2))\n",
        "    self.conv8 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm7 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv9 = tf.keras.layers.Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    E, I = inputs\n",
        "    C, phi = self.canet(E)\n",
        "\n",
        "    X = self.conv1(I)\n",
        "    X = tf.nn.relu(X)\n",
        "    \n",
        "    X = self.conv2(X)\n",
        "    X = self.batchnorm1(X)\n",
        "    X = tf.nn.relu(X)\n",
        "\n",
        "    X = self.conv3(X)\n",
        "    X = self.batchnorm2(X)\n",
        "    X = tf.nn.relu(X)\n",
        "\n",
        "    C = K.expand_dims(C, axis = 1)\n",
        "    C = K.expand_dims(C, axis = 1)\n",
        "    C = K.tile(C, [1, 16, 16, 1])\n",
        "    J = K.concatenate([C, X], axis = 3)\n",
        "\n",
        "    X = self.conv4(X)\n",
        "    X = self.batchnorm3(X)\n",
        "    X = tf.nn.relu(X)\n",
        "\n",
        "    X = self.resblock1(X)\n",
        "    X = self.resblock2(X)\n",
        "    X = self.resblock3(X)\n",
        "    X = self.resblock4(X)\n",
        "\n",
        "    X = self.upsamp1(X)\n",
        "    X = self.conv5(X)\n",
        "    X = self.batchnorm4(X)\n",
        "    X = tf.nn.relu(X)\n",
        "    \n",
        "    X = self.upsamp2(X)\n",
        "    X = self.conv6(X)\n",
        "    X = self.batchnorm5(X)\n",
        "    X = tf.nn.relu(X)\n",
        "    \n",
        "    X = self.upsamp3(X)\n",
        "    X = self.conv7(X)\n",
        "    X = self.batchnorm6(X)\n",
        "    X = tf.nn.relu(X)\n",
        "    \n",
        "    X = self.upsamp4(X)\n",
        "    X = self.conv8(X)\n",
        "    X = self.batchnorm7(X)\n",
        "    X = tf.nn.relu(X)\n",
        "    \n",
        "    X = self.conv9(X)\n",
        "    return tf.nn.tanh(X), phi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2ODvpdkSGY-"
      },
      "source": [
        "class Stage2Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage2Discriminator, self).__init__()\n",
        "    self.embed = EmbeddingCompressor()\n",
        "    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv5 = tf.keras.layers.Conv2D(filters = 1024, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv6 = tf.keras.layers.Conv2D(filters = 2048, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm5 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv7 = tf.keras.layers.Conv2D(filters = 1024, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm6 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv8 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm7 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv9 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm8 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv10 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm9 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv11 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm10 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "\n",
        "    self.conv12 = tf.keras.layers.Conv2D(filters = 64*8, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "    self.batchnorm11 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "    self.conv13 = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 4, padding = \"valid\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    I, E = inputs\n",
        "    T = self.embed(E)\n",
        "    T = self.reshape(T)\n",
        "    T = tf.tile(T, (1, 4, 4, 1))\n",
        "\n",
        "    X = self.conv1(I)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "\n",
        "    X = self.conv2(X)\n",
        "    X = self.batchnorm1(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "    \n",
        "    X = self.conv3(X)\n",
        "    X = self.batchnorm2(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "    \n",
        "    X = self.conv4(X)\n",
        "    X = self.batchnorm3(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "    \n",
        "    X = self.conv5(X)\n",
        "    X = self.batchnorm4(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "   \n",
        "    X = self.conv6(X)\n",
        "    X = self.batchnorm5(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "    \n",
        "    X = self.conv7(X)\n",
        "    X = self.batchnorm6(X)\n",
        "    X = tf.nn.leaky_relu(X)\n",
        "    \n",
        "    X = self.conv8(X)\n",
        "    X = self.batchnorm7(X)\n",
        "\n",
        "    Y = self.conv9(X)\n",
        "    Y = self.batchnorm8(Y)\n",
        "    Y = tf.nn.leaky_relu(Y)\n",
        "\n",
        "    Y = self.conv10(Y)\n",
        "    Y = self.batchnorm9(Y)\n",
        "    Y = tf.nn.leaky_relu(Y)\n",
        "\n",
        "    Y = self.conv11(Y)\n",
        "    Y = self.batchnorm10(Y)\n",
        "\n",
        "    A = tf.keras.layers.Add()([X, Y])\n",
        "    A = tf.nn.leaky_relu(A)\n",
        "\n",
        "    merged_input = tf.keras.layers.concatenate([A, T])\n",
        "\n",
        "    Z = self.conv12(merged_input)\n",
        "    Z = self.batchnorm11(Z)\n",
        "    Z = tf.nn.leaky_relu(Z)\n",
        "    \n",
        "    Z = self.conv13(Z)\n",
        "    return tf.squeeze(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJnLBzAKKrc3"
      },
      "source": [
        "class Stage2Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage2Model, self).__init__()\n",
        "    self.stage1_generator = Stage1Generator()\n",
        "    self.stage1_generator.compile(loss = \"mse\", optimizer = \"adam\")\n",
        "    self.stage1_generator.load_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_generator_600.ckpt\").expect_partial()\n",
        "    self.stage2_generator = Stage2Generator()\n",
        "    self.stage2_discriminator = Stage2Discriminator()\n",
        "\n",
        "   \n",
        "    \n",
        "  ### lr\n",
        "  def train(self, train_ds, batch_size = 16, num_epochs = 1200, z_dim = 100, stage1_generator_lr = 0.0002, stage1_discriminator_lr = 0.0002):\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    \n",
        "    for epoch in range(0, num_epochs): ### 170\n",
        "      print(\"Epoch %d/%d:\\n [\"%(epoch + 1, num_epochs), end = \"\")\n",
        "      start_time = time.time()\n",
        "      if epoch % 100 == 0:\n",
        "        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n",
        "        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n",
        "    \n",
        "      generator_loss_log = []\n",
        "      discriminator_loss_log = []\n",
        "      steps_per_epoch = 125\n",
        "      batch_iter = iter(train_ds)\n",
        "      for i in range(steps_per_epoch):\n",
        "        if i % 5 == 0:\n",
        "          print(\"=\", end = \"\")\n",
        "        hr_image_batch, embedding_batch = next(batch_iter)\n",
        "        z_noise = tf.random.normal((batch_size, z_dim))\n",
        "\n",
        "        mismatched_images = tf.roll(hr_image_batch, shift = 1, axis = 0)\n",
        "\n",
        "        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n",
        "        fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "        mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "\n",
        "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "          lr_fake_images, _ = self.stage1_generator([embedding_batch, z_noise])\n",
        "          hr_fake_images, phi = self.stage2_generator([embedding_batch, lr_fake_images])\n",
        "          real_logits = self.stage2_discriminator([hr_image_batch, embedding_batch])\n",
        "          fake_logits = self.stage2_discriminator([hr_fake_images, embedding_batch])\n",
        "          mismatched_logits = self.stage2_discriminator([mismatched_images, embedding_batch])\n",
        "          \n",
        "          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n",
        "          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "          generator_loss = l_sup + 2.0*l_klreg\n",
        "          \n",
        "          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
        "          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
        "          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
        "          discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n",
        "        \n",
        "        generator_gradients = generator_tape.gradient(generator_loss, self.stage2_generator.trainable_variables)\n",
        "        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage2_discriminator.trainable_variables)\n",
        "        \n",
        "        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage2_generator.trainable_variables))\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage2_discriminator.trainable_variables))\n",
        "        \n",
        "        generator_loss_log.append(generator_loss)\n",
        "        discriminator_loss_log.append(discriminator_loss)\n",
        "        \n",
        "      end_time = time.time()\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "        epoch_time = end_time - start_time\n",
        "        template = \"] - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "        print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "      if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "        save_path = \"/content/drive/My Drive/Text to Image/hr_results/epoch_\" + str(epoch + 1)\n",
        "        temp_embeddings = None\n",
        "        for _, embeddings in train_ds:\n",
        "          temp_embeddings = embeddings.numpy()\n",
        "          break\n",
        "        if os.path.exists(save_path) == False:\n",
        "          os.makedirs(save_path)\n",
        "        temp_batch_size = 10\n",
        "        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n",
        "        temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
        "        fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise])\n",
        "        for i, image in enumerate(fake_images):\n",
        "          image = 127.5*image + 127.5\n",
        "          image = image.numpy().astype('uint8')\n",
        "          cv2.imwrite(save_path + \"/gen_%d.png\"%(i), image)\n",
        "        self.stage2_generator.save_weights(\"/content/drive/My Drive/Text to Image/hr_model_checkpoints/stage2_generator_\" + str(epoch + 1) + \".ckpt\")\n",
        "        self.stage2_discriminator.save_weights(\"/content/drive/My Drive/Text to Image/hr_model_checkpoints/stage2_discriminator_\" + str(epoch + 1) + \".ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT6BsHTdMgeO"
      },
      "source": [
        "dataset = Dataset(image_size = (256, 256))\n",
        "train_ds = dataset.get_train_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEk5LubAVhq0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52dc8ae5-dc8e-4a14-8c81-1cde50da0f7e"
      },
      "source": [
        "stage2_model = Stage2Model()\n",
        "stage2_model.train(train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1200:\n",
            " [=========================] - generator_loss: 1.7323 - discriminator_loss: 0.6168 - epoch_time: 877.81 s\n",
            "Epoch 2/1200:\n",
            " [=========================] - generator_loss: 1.4515 - discriminator_loss: 0.6344 - epoch_time: 571.64 s\n",
            "Epoch 3/1200:\n",
            " [=========================] - generator_loss: 1.5055 - discriminator_loss: 0.6228 - epoch_time: 434.90 s\n",
            "Epoch 4/1200:\n",
            " [=========================] - generator_loss: 2.1972 - discriminator_loss: 0.5870 - epoch_time: 344.37 s\n",
            "Epoch 5/1200:\n",
            " [=========================] - generator_loss: 1.8523 - discriminator_loss: 0.6191 - epoch_time: 254.10 s\n",
            "Epoch 6/1200:\n",
            " [=========================] - generator_loss: 2.0137 - discriminator_loss: 0.5890 - epoch_time: 212.04 s\n",
            "Epoch 7/1200:\n",
            " [=========================] - generator_loss: 2.7649 - discriminator_loss: 0.5414 - epoch_time: 165.27 s\n",
            "Epoch 8/1200:\n",
            " [=========================] - generator_loss: 2.8453 - discriminator_loss: 0.5315 - epoch_time: 137.99 s\n",
            "Epoch 9/1200:\n",
            " [=========================] - generator_loss: 2.8508 - discriminator_loss: 0.5269 - epoch_time: 135.19 s\n",
            "Epoch 10/1200:\n",
            " [=========================] - generator_loss: 2.8794 - discriminator_loss: 0.5163 - epoch_time: 135.43 s\n",
            "Epoch 11/1200:\n",
            " [=========================] - generator_loss: 2.8683 - discriminator_loss: 0.5071 - epoch_time: 135.30 s\n",
            "Epoch 12/1200:\n",
            " [=========================] - generator_loss: 2.9216 - discriminator_loss: 0.5078 - epoch_time: 136.43 s\n",
            "Epoch 13/1200:\n",
            " [=========================] - generator_loss: 2.8624 - discriminator_loss: 0.4980 - epoch_time: 136.42 s\n",
            "Epoch 14/1200:\n",
            " [=========================] - generator_loss: 2.8865 - discriminator_loss: 0.4821 - epoch_time: 136.76 s\n",
            "Epoch 15/1200:\n",
            " [=========================] - generator_loss: 2.8980 - discriminator_loss: 0.4796 - epoch_time: 137.32 s\n",
            "Epoch 16/1200:\n",
            " [=========================] - generator_loss: 2.8699 - discriminator_loss: 0.4809 - epoch_time: 136.67 s\n",
            "Epoch 17/1200:\n",
            " [=========================] - generator_loss: 2.8609 - discriminator_loss: 0.4782 - epoch_time: 136.23 s\n",
            "Epoch 18/1200:\n",
            " [=========================] - generator_loss: 2.8703 - discriminator_loss: 0.4694 - epoch_time: 136.48 s\n",
            "Epoch 19/1200:\n",
            " [=========================] - generator_loss: 2.8643 - discriminator_loss: 0.4636 - epoch_time: 135.94 s\n",
            "Epoch 20/1200:\n",
            " [=========================] - generator_loss: 2.8916 - discriminator_loss: 0.4638 - epoch_time: 135.95 s\n",
            "Epoch 21/1200:\n",
            " [=========================] - generator_loss: 2.9016 - discriminator_loss: 0.4639 - epoch_time: 136.05 s\n",
            "Epoch 22/1200:\n",
            " [=========================] - generator_loss: 2.8833 - discriminator_loss: 0.4575 - epoch_time: 136.14 s\n",
            "Epoch 23/1200:\n",
            " [=========================] - generator_loss: 2.8780 - discriminator_loss: 0.4526 - epoch_time: 136.57 s\n",
            "Epoch 24/1200:\n",
            " [=========================] - generator_loss: 2.8847 - discriminator_loss: 0.4500 - epoch_time: 136.58 s\n",
            "Epoch 25/1200:\n",
            " [=========================] - generator_loss: 2.8821 - discriminator_loss: 0.4498 - epoch_time: 136.34 s\n",
            "Epoch 26/1200:\n",
            " [=========================] - generator_loss: 2.8887 - discriminator_loss: 0.4423 - epoch_time: 136.35 s\n",
            "Epoch 27/1200:\n",
            " [=========================] - generator_loss: 2.8653 - discriminator_loss: 0.4466 - epoch_time: 136.85 s\n",
            "Epoch 28/1200:\n",
            " [=========================] - generator_loss: 2.9029 - discriminator_loss: 0.4462 - epoch_time: 136.74 s\n",
            "Epoch 29/1200:\n",
            " [=========================] - generator_loss: 2.8598 - discriminator_loss: 0.4367 - epoch_time: 136.67 s\n",
            "Epoch 30/1200:\n",
            " [=========================] - generator_loss: 2.8818 - discriminator_loss: 0.4372 - epoch_time: 136.65 s\n",
            "Epoch 31/1200:\n",
            " [=========================] - generator_loss: 2.8646 - discriminator_loss: 0.4434 - epoch_time: 136.78 s\n",
            "Epoch 32/1200:\n",
            " [=========================] - generator_loss: 2.8740 - discriminator_loss: 0.4404 - epoch_time: 136.51 s\n",
            "Epoch 33/1200:\n",
            " [=========================] - generator_loss: 2.9336 - discriminator_loss: 0.4425 - epoch_time: 136.19 s\n",
            "Epoch 34/1200:\n",
            " [=========================] - generator_loss: 2.8926 - discriminator_loss: 0.4278 - epoch_time: 136.60 s\n",
            "Epoch 35/1200:\n",
            " [=========================] - generator_loss: 2.8836 - discriminator_loss: 0.4286 - epoch_time: 136.75 s\n",
            "Epoch 36/1200:\n",
            " [=========================] - generator_loss: 2.8943 - discriminator_loss: 0.4238 - epoch_time: 136.67 s\n",
            "Epoch 37/1200:\n",
            " [=========================] - generator_loss: 2.8886 - discriminator_loss: 0.4264 - epoch_time: 136.42 s\n",
            "Epoch 38/1200:\n",
            " [=========================] - generator_loss: 2.8828 - discriminator_loss: 0.4202 - epoch_time: 136.28 s\n",
            "Epoch 39/1200:\n",
            " [=========================] - generator_loss: 2.8622 - discriminator_loss: 0.4232 - epoch_time: 136.23 s\n",
            "Epoch 40/1200:\n",
            " [=========================] - generator_loss: 2.8885 - discriminator_loss: 0.4147 - epoch_time: 136.13 s\n",
            "Epoch 41/1200:\n",
            " [=========================] - generator_loss: 2.8708 - discriminator_loss: 0.4160 - epoch_time: 136.32 s\n",
            "Epoch 42/1200:\n",
            " [=========================] - generator_loss: 2.8836 - discriminator_loss: 0.4161 - epoch_time: 136.67 s\n",
            "Epoch 43/1200:\n",
            " [=========================] - generator_loss: 2.8612 - discriminator_loss: 0.4200 - epoch_time: 137.46 s\n",
            "Epoch 44/1200:\n",
            " [=========================] - generator_loss: 2.8791 - discriminator_loss: 0.4141 - epoch_time: 136.88 s\n",
            "Epoch 45/1200:\n",
            " [=========================] - generator_loss: 2.8600 - discriminator_loss: 0.4053 - epoch_time: 136.92 s\n",
            "Epoch 46/1200:\n",
            " [=========================] - generator_loss: 2.8732 - discriminator_loss: 0.4137 - epoch_time: 137.25 s\n",
            "Epoch 47/1200:\n",
            " [=========================] - generator_loss: 2.8739 - discriminator_loss: 0.4092 - epoch_time: 136.75 s\n",
            "Epoch 48/1200:\n",
            " [=========================] - generator_loss: 2.8488 - discriminator_loss: 0.4030 - epoch_time: 136.44 s\n",
            "Epoch 49/1200:\n",
            " [=========================] - generator_loss: 2.8751 - discriminator_loss: 0.4012 - epoch_time: 135.43 s\n",
            "Epoch 50/1200:\n",
            " [=========================] - generator_loss: 2.8611 - discriminator_loss: 0.3978 - epoch_time: 135.30 s\n",
            "Epoch 51/1200:\n",
            " [=========================] - generator_loss: 2.8839 - discriminator_loss: 0.3975 - epoch_time: 135.71 s\n",
            "Epoch 52/1200:\n",
            " [=========================] - generator_loss: 2.8558 - discriminator_loss: 0.4000 - epoch_time: 135.51 s\n",
            "Epoch 53/1200:\n",
            " [=========================] - generator_loss: 2.8782 - discriminator_loss: 0.3922 - epoch_time: 135.19 s\n",
            "Epoch 54/1200:\n",
            " [=========================] - generator_loss: 2.8620 - discriminator_loss: 0.3944 - epoch_time: 135.07 s\n",
            "Epoch 55/1200:\n",
            " [=========================] - generator_loss: 2.8977 - discriminator_loss: 0.3776 - epoch_time: 135.07 s\n",
            "Epoch 56/1200:\n",
            " [=========================] - generator_loss: 2.8600 - discriminator_loss: 0.3923 - epoch_time: 134.85 s\n",
            "Epoch 57/1200:\n",
            " [=========================] - generator_loss: 2.8631 - discriminator_loss: 0.3856 - epoch_time: 135.32 s\n",
            "Epoch 58/1200:\n",
            " [=========================] - generator_loss: 2.8831 - discriminator_loss: 0.3780 - epoch_time: 134.92 s\n",
            "Epoch 59/1200:\n",
            " [=========================] - generator_loss: 2.8574 - discriminator_loss: 0.3816 - epoch_time: 135.11 s\n",
            "Epoch 60/1200:\n",
            " [=========================] - generator_loss: 2.8821 - discriminator_loss: 0.3762 - epoch_time: 135.13 s\n",
            "Epoch 61/1200:\n",
            " [=========================] - generator_loss: 2.8859 - discriminator_loss: 0.3718 - epoch_time: 135.38 s\n",
            "Epoch 62/1200:\n",
            " [=========================] - generator_loss: 2.8850 - discriminator_loss: 0.3748 - epoch_time: 135.14 s\n",
            "Epoch 63/1200:\n",
            " [=========================] - generator_loss: 2.8610 - discriminator_loss: 0.3748 - epoch_time: 135.15 s\n",
            "Epoch 64/1200:\n",
            " [=========================] - generator_loss: 2.8462 - discriminator_loss: 0.3685 - epoch_time: 135.11 s\n",
            "Epoch 65/1200:\n",
            " [=========================] - generator_loss: 2.8504 - discriminator_loss: 0.3718 - epoch_time: 135.03 s\n",
            "Epoch 66/1200:\n",
            " [=========================] - generator_loss: 2.8669 - discriminator_loss: 0.3670 - epoch_time: 135.35 s\n",
            "Epoch 67/1200:\n",
            " [=========================] - generator_loss: 2.8584 - discriminator_loss: 0.3674 - epoch_time: 135.08 s\n",
            "Epoch 68/1200:\n",
            " [=========================] - generator_loss: 2.8432 - discriminator_loss: 0.3640 - epoch_time: 135.24 s\n",
            "Epoch 69/1200:\n",
            " [=========================] - generator_loss: 2.8608 - discriminator_loss: 0.3670 - epoch_time: 134.97 s\n",
            "Epoch 70/1200:\n",
            " [=========================] - generator_loss: 2.8932 - discriminator_loss: 0.3568 - epoch_time: 134.74 s\n",
            "Epoch 71/1200:\n",
            " [=========================] - generator_loss: 2.8680 - discriminator_loss: 0.3587 - epoch_time: 134.75 s\n",
            "Epoch 72/1200:\n",
            " [=========================] - generator_loss: 2.8686 - discriminator_loss: 0.3553 - epoch_time: 135.17 s\n",
            "Epoch 73/1200:\n",
            " [=========================] - generator_loss: 2.8764 - discriminator_loss: 0.3513 - epoch_time: 135.15 s\n",
            "Epoch 74/1200:\n",
            " [=========================] - generator_loss: 2.8404 - discriminator_loss: 0.3479 - epoch_time: 134.77 s\n",
            "Epoch 75/1200:\n",
            " [=========================] - generator_loss: 2.8778 - discriminator_loss: 0.3509 - epoch_time: 135.14 s\n",
            "Epoch 76/1200:\n",
            " [=========================] - generator_loss: 2.8801 - discriminator_loss: 0.3430 - epoch_time: 135.08 s\n",
            "Epoch 77/1200:\n",
            " [=========================] - generator_loss: 2.8589 - discriminator_loss: 0.3413 - epoch_time: 135.00 s\n",
            "Epoch 78/1200:\n",
            " [=========================] - generator_loss: 2.8821 - discriminator_loss: 0.3416 - epoch_time: 134.50 s\n",
            "Epoch 79/1200:\n",
            " [=========================] - generator_loss: 2.8701 - discriminator_loss: 0.3400 - epoch_time: 134.58 s\n",
            "Epoch 80/1200:\n",
            " [=========================] - generator_loss: 2.8771 - discriminator_loss: 0.3385 - epoch_time: 134.65 s\n",
            "Epoch 81/1200:\n",
            " [=========================] - generator_loss: 2.8830 - discriminator_loss: 0.3376 - epoch_time: 134.77 s\n",
            "Epoch 82/1200:\n",
            " [=========================] - generator_loss: 2.8985 - discriminator_loss: 0.3441 - epoch_time: 134.84 s\n",
            "Epoch 83/1200:\n",
            " [=========================] - generator_loss: 2.8872 - discriminator_loss: 0.3386 - epoch_time: 134.78 s\n",
            "Epoch 84/1200:\n",
            " [=========================] - generator_loss: 2.8429 - discriminator_loss: 0.3316 - epoch_time: 135.22 s\n",
            "Epoch 85/1200:\n",
            " [=========================] - generator_loss: 2.8697 - discriminator_loss: 0.3307 - epoch_time: 133.74 s\n",
            "Epoch 86/1200:\n",
            " [=========================] - generator_loss: 2.8836 - discriminator_loss: 0.3336 - epoch_time: 132.80 s\n",
            "Epoch 87/1200:\n",
            " [=========================] - generator_loss: 2.8774 - discriminator_loss: 0.3228 - epoch_time: 132.54 s\n",
            "Epoch 88/1200:\n",
            " [=========================] - generator_loss: 2.8656 - discriminator_loss: 0.3243 - epoch_time: 132.55 s\n",
            "Epoch 89/1200:\n",
            " [=========================] - generator_loss: 2.8567 - discriminator_loss: 0.3247 - epoch_time: 132.53 s\n",
            "Epoch 90/1200:\n",
            " [=========================] - generator_loss: 2.8827 - discriminator_loss: 0.3259 - epoch_time: 132.37 s\n",
            "Epoch 91/1200:\n",
            " [=========================] - generator_loss: 2.8763 - discriminator_loss: 0.3192 - epoch_time: 133.23 s\n",
            "Epoch 92/1200:\n",
            " [=========================] - generator_loss: 2.8543 - discriminator_loss: 0.3180 - epoch_time: 132.78 s\n",
            "Epoch 93/1200:\n",
            " [=========================] - generator_loss: 2.8735 - discriminator_loss: 0.3217 - epoch_time: 132.59 s\n",
            "Epoch 94/1200:\n",
            " [=========================] - generator_loss: 2.8766 - discriminator_loss: 0.3194 - epoch_time: 132.94 s\n",
            "Epoch 95/1200:\n",
            " [=========================] - generator_loss: 2.8386 - discriminator_loss: 0.3120 - epoch_time: 132.72 s\n",
            "Epoch 96/1200:\n",
            " [==============="
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9aDTZqZ5RVT"
      },
      "source": [
        "class Stage2Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage2Model, self).__init__()\n",
        "    self.stage1_generator = Stage1Generator()\n",
        "    # self.stage1_generator.compile(loss = 'mse', optimizer = \"adam\")\n",
        "    # self.stage1_generator.train_on_batch([K.ones((1, 1024)), K.ones((1, 100))], [K.ones((1, 64, 64, 3)), K.ones((1, 256))])\n",
        "    self.stage1_generator.load_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_generator_600.ckpt\")\n",
        "    self.stage2_generator = Stage2Generator()\n",
        "    self.stage2_discriminator = Stage2Discriminator()\n",
        "\n",
        "  def train(self, X_train, batch_size = 64, num_epochs = 1200, z_dim = 100, stage1_generator_lr = 0.0004, stage1_discriminator_lr = 0.0004):\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"Epoch %d/%d:\\n [\"%(epoch + 1, num_epochs), end = \"\")\n",
        "      start_time = time.time()\n",
        "      if epoch % 50 == 0:\n",
        "        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n",
        "        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n",
        "    \n",
        "      generator_loss_log = []\n",
        "      discriminator_loss_log = []\n",
        "      steps_per_epoch = 125\n",
        "      batch_iter = iter(train_ds)\n",
        "      for i in range(steps_per_epoch):\n",
        "        if i % 5 == 0:\n",
        "          print(\"=\", end = \"\")\n",
        "        hr_image_batch, embedding_batch = next(batch_iter)\n",
        "        z_noise = tf.random.normal((batch_size, z_dim))\n",
        "\n",
        "        mismatched_images = tf.roll(image_batch, shift = 1, axis = 0)\n",
        "\n",
        "        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n",
        "        fake_labels = tf.zeros(shape = (batch_size, ))\n",
        "        mismatched_labels = tf.zeros(shape = (batch_size, ))\n",
        "        disc_labels=tf.concat([real_labels,fake_labels],axis=0)\n",
        "\n",
        "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "          lr_fake_images, _ = self.stage1_generator([embedding_batch, z_noise])\n",
        "          hr_fake_images, phi = self.stage2_generator([embedding_batch, lr_fake_images])\n",
        "          real_logits = self.stage2_discriminator([hr_image_batch, embedding_batch])\n",
        "          fake_logits = self.stage2_discriminator([hr_fake_images, embedding_batch])\n",
        "          mismatched_logits = self.stage2_discriminator([mismatched_images, embedding_batch])\n",
        "          \n",
        "          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n",
        "          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "          generator_loss = l_sup + 2*l_klreg\n",
        "          \n",
        "          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
        "          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
        "          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
        "          discriminator_loss = tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n",
        "        \n",
        "        generator_gradients = generator_tape.gradient(generator_loss, self.stage2_generator.trainable_variables)\n",
        "        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage2_discriminator.trainable_variables)\n",
        "        \n",
        "        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage2_generator.trainable_variables))\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage2_discriminator.trainable_variables))\n",
        "        \n",
        "        generator_loss_log.append(generator_loss)\n",
        "        discriminator_loss_log.append(discriminator_loss)\n",
        "        \n",
        "      end_time = time.time()\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "        epoch_time = end_time - start_time\n",
        "        template = \"Epoch {:03d}: generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "        print(template.format(epoch + 1, tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "      if epoch % 50 == 0 or epoch == num_epochs - 1:\n",
        "        save_path = \"/content/drive/My Drive/Text to Image/hr_results/epoch_\" + str(epoch + 1)\n",
        "        if os.path.exists(save_path) == False:\n",
        "          os.makedirs(save_path)\n",
        "        temp_batch_size = 10\n",
        "        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n",
        "        temp_embedding_batch = train_embeddings[0:temp_batch_size]\n",
        "        fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise])\n",
        "        for i, image in enumerate(fake_images):\n",
        "          image = 127.5*image + 127.5\n",
        "          image = image.numpy().astype('uint8')\n",
        "          cv2.imwrite(save_path + \"/gen_%d.png\"%(i), image)\n",
        "        self.stage2_generator.save_weights(\"/content/drive/My Drive/Text to Image/stage2_model_weights/stage1_generator_\" + str(epoch + 1) + \".ckpt\")\n",
        "        self.stage2_discriminator.save_weights(\"/content/drive/My Drive/Text to Image/stage2_model_weights/stage1_discriminator_\" + str(epoch + 1) + \".ckpt\")\n",
        "   \n",
        "    def generate_image(self, embedding):\n",
        "      self.stage2_generator.compile(loss = \"mse\", optimizer = \"adam\")\n",
        "      self.stage2_generator.load_weights(\"/content/drive/My Drive/Text to Image/lr_model_checkpoints/stage1_generator_600.ckpt\").expect_partial()\n",
        "      z_noise = tf.random.normal((batch_size, z_dim))\n",
        "      generated_image = self.stage1_generator([embedding, z_noise])\n",
        "     \n",
        "      return generated_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7rLSPg1Hgxo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkHrakCkHgt8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57uiXeDZHgq4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs2DwQjfHgn7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftCvzBchsAc"
      },
      "source": [
        "class ResBlock(tf.keras.Model):\n",
        "  def __init__(self,filters,filters_shapes,pads=\"same\",strides=1,use_bias=True):#(3,3)\n",
        "    super(ResBlock,self).__init__()\n",
        "    filter1, filter2, filter3 = filters\n",
        "    filter1_shape, filter2_shape, filter3_shape = filters_shapes\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filter1, filter1_shape,strides=strides,padding=\"same\",use_bias=use_bias)\n",
        "    self.batchNorm1= tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    self.conv2= tf.keras.layers.Conv2D(filter2, filter2_shape,strides=strides,padding=\"same\",use_bias=use_bias)\n",
        "    self.batchNorm2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    self.conv3= tf.keras.layers.Conv2D(filter3, filter3_shape,strides=strides,padding=\"same\",use_bias=use_bias)\n",
        "    self.batchNorm3 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self,input_tensor,is_training=True):\n",
        "    x=self.conv1(input_tensor)\n",
        "    x=self.batchNorm1(x,training=is_training)\n",
        "    x=tf.nn.relu(x)\n",
        "\n",
        "    x=self.conv2(x)\n",
        "    x=self.batchNorm2(x,training=is_training)\n",
        "    x=tf.nn.relu(x)\n",
        "\n",
        "    x=self.conv3(x)\n",
        "    x=self.batchNorm3(x,training=is_training)\n",
        "    \n",
        "\n",
        "    x = tf.add(x,input_tensor)\n",
        "    return tf.nn.relu(x) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BnqGJG54A9O"
      },
      "source": [
        "class Stage1Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage1Model, self).__init__()\n",
        "    self.stage1_generator = Stage1Generator()\n",
        "    self.stage1_discriminator = Stage1Discriminator()\n",
        "\n",
        "  def train(self, X_train, train_embeddings, batch_size = 64, num_epochs = 600, z_dim = 100, c_dim = 128, stage1_generator_lr = 0.0004, stage1_discriminator_lr = 0.0004):\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      if epoch % 50 == 0:\n",
        "        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n",
        "        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n",
        "    \n",
        "      generator_loss_log = []\n",
        "      discriminator_loss_log = []\n",
        "      steps_per_epoch = X_train.shape[0] // batch_size\n",
        "      for i in range(steps_per_epoch):\n",
        "        image_batch = X_train[i*batch_size:(i + 1)*batch_size]\n",
        "        image_batch = (image_batch - 127.5) / 127.5\n",
        "        embedding_batch = train_embeddings[i*batch_size:(i + 1)*batch_size]\n",
        "        z_noise = tf.random.normal((batch_size, z_dim))\n",
        "\n",
        "        mismatched_images = tf.roll(image_batch, shift = 1, axis = 0)\n",
        "\n",
        "        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n",
        "        fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "        mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "\n",
        "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "          fake_images, phi = self.stage1_generator([embedding_batch, z_noise])\n",
        "          \n",
        "          real_logits = self.stage1_discriminator([image_batch, embedding_batch])\n",
        "          fake_logits = self.stage1_discriminator([fake_images, embedding_batch])\n",
        "          mismatched_logits = self.stage1_discriminator([mismatched_images, embedding_batch])\n",
        "          \n",
        "          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n",
        "          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "          generator_loss = l_sup + 2.0*l_klreg\n",
        "          \n",
        "          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
        "          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
        "          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
        "          discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n",
        "        \n",
        "        generator_gradients = generator_tape.gradient(generator_loss, self.stage1_generator.trainable_variables+self.stage1_discriminator.trainable_variables)\n",
        "        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage1_discriminator.trainable_variables)\n",
        "        \n",
        "        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage1_generator.trainable_variables))\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage1_discriminator.trainable_variables))\n",
        "        \n",
        "        generator_loss_log.append(generator_loss)\n",
        "        discriminator_loss_log.append(discriminator_loss)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "        epoch_time = end_time - start_time\n",
        "        template = \"Epoch {:03d}: generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "        print(template.format(epoch + 1, tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "      if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "        save_path = \"/content/drive/My Drive/Text to Image/lr_results/epoch_\" + str(epoch + 1)\n",
        "        if os.path.exists(save_path) == False:\n",
        "          os.makedirs(save_path)\n",
        "        temp_batch_size = 10\n",
        "        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n",
        "        temp_embedding_batch = train_embeddings[0:temp_batch_size]\n",
        "        fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise])\n",
        "        for i, image in enumerate(fake_images):\n",
        "          image = 127.5*image + 127.5\n",
        "          image = image.numpy().astype('uint8')\n",
        "          cv2.imwrite(save_path + \"/gen_%d.png\"%(i), image)\n",
        "        self.stage1_generator.save_weights(\"/content/drive/My Drive/Text to Image/model_weights/stage1_generator_\" + str(epoch + 1) + \".h5\")\n",
        "        self.stage1_discriminator.save_weights(\"/content/drive/My Drive/Text to Image/model_weights/stage1_discriminator_\" + str(epoch + 1) + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC5GROO34AzJ"
      },
      "source": [
        "if os.path.exists('/content/drive/My Drive/Text to Image/utility_data/X_train_lr.npy'):\n",
        "  X_train_lr = np.load('/content/drive/My Drive/Text to Image/utility_data/X_train_lr.npy')\n",
        "  Y_train_lr = np.load('/content/drive/My Drive/Text to Image/utility_data/Y_train_lr.npy')\n",
        "  X_test_lr = np.load('/content/drive/My Drive/Text to Image/utility_data/X_test_lr.npy')\n",
        "  Y_test_lr = np.load('/content/drive/My Drive/Text to Image/utility_data/Y_test_lr.npy')\n",
        "  train_embeddings = np.load('/content/drive/My Drive/Text to Image/utility_data/train_embeddings.npy')\n",
        "  test_embeddings = np.load('/content/drive/My Drive/Text to Image/utility_data/test_embeddings.npy')\n",
        "  X_train_lr = X_train_lr.astype('float32')\n",
        "  X_test_lr = X_test_lr.astype('float32')\n",
        "\n",
        "else: \n",
        "  data_dir = \"/content/drive/My Drive/Text to Image/dataset/birds\"\n",
        "  train_dir = data_dir + \"/train\"\n",
        "  test_dir = data_dir + \"/test\"\n",
        "  train_embeddings_path = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "  test_embeddings_path = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "  train_filenames_path = train_dir + \"/filenames.pickle\"\n",
        "  test_filenames_path = test_dir + \"/filenames.pickle\"\n",
        "  train_class_info_path = train_dir + \"/class_info.pickle\"\n",
        "  test_class_info_path = test_dir + \"/class_info.pickle\"\n",
        "  cub_dataset_dir = \"/content/drive/My Drive/Text to Image/dataset/CUB_200_2011/CUB_200_2011\"\n",
        "  bounding_boxes_path = cub_dataset_dir + \"/bounding_boxes.txt\"\n",
        "  image_names_path = cub_dataset_dir + \"/images.txt\"\n",
        "\n",
        "  train_loader = DataLoader(train_filenames_path, train_class_info_path, train_embeddings_path, bounding_boxes_path, image_names_path, image_size = (64, 64))  \n",
        "  X_train_lr, Y_train_lr, train_embeddings = train_loader.load_dataset()\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/X_train_lr', X_train_lr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/Y_train_lr', Y_train_lr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/train_embeddings', train_embeddings)\n",
        "\n",
        "  test_loader = DataLoader(test_filenames_path, test_class_info_path, test_embeddings_path, bounding_boxes_path, image_names_path, image_size = (64, 64))  \n",
        "  X_test_lr, Y_test_lr, test_embeddings = test_loader.load_dataset()\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/X_test_lr', X_test_lr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/Y_test_lr', Y_test_lr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/test_embeddings', test_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV6O-xrQAJ3B"
      },
      "source": [
        "if os.path.exists('/content/drive/My Drive/Text to Image/utility_data/X_train_hr.npy'):\n",
        "  X_train_hr = np.load('/content/drive/My Drive/Text to Image/utility_data/X_train_hr.npy')\n",
        "  Y_train_hr = np.load('/content/drive/My Drive/Text to Image/utility_data/Y_train_hr.npy')\n",
        "  X_test_hr = np.load('/content/drive/My Drive/Text to Image/utility_data/X_test_hr.npy')\n",
        "  Y_test_hr = np.load('/content/drive/My Drive/Text to Image/utility_data/Y_test_hr.npy')\n",
        "  train_embeddings = np.load('/content/drive/My Drive/Text to Image/utility_data/train_embeddings.npy')\n",
        "  test_embeddings = np.load('/content/drive/My Drive/Text to Image/utility_data/test_embeddings.npy')\n",
        "\n",
        "else:\n",
        "  data_dir = \"/content/drive/My Drive/Text to Image/dataset/birds\"\n",
        "  train_dir = data_dir + \"/train\"\n",
        "  test_dir = data_dir + \"/test\"\n",
        "  train_embeddings_path = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "  test_embeddings_path = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "  train_filenames_path = train_dir + \"/filenames.pickle\"\n",
        "  test_filenames_path = test_dir + \"/filenames.pickle\"\n",
        "  train_class_info_path = train_dir + \"/class_info.pickle\"\n",
        "  test_class_info_path = test_dir + \"/class_info.pickle\"\n",
        "  cub_dataset_dir = \"/content/drive/My Drive/Text to Image/dataset/CUB_200_2011/CUB_200_2011\"\n",
        "  bounding_boxes_path = cub_dataset_dir + \"/bounding_boxes.txt\"\n",
        "  image_names_path = cub_dataset_dir + \"/images.txt\"\n",
        "\n",
        "  train_loader = DataLoader(train_filenames_path, train_class_info_path, train_embeddings_path, bounding_boxes_path, image_names_path, image_size = (256, 256))  \n",
        "  X_train_hr, Y_train_hr, train_embeddings = train_loader.load_dataset()\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/X_train_hr', X_train_hr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/Y_train_hr', Y_train_hr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/train_embeddings', train_embeddings)\n",
        "\n",
        "  test_loader = DataLoader(test_filenames_path, test_class_info_path, test_embeddings_path, bounding_boxes_path, image_names_path, image_size = (256, 256))  \n",
        "  X_test_lr, Y_test_lr, test_embeddings = test_loader.load_dataset()\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/X_test_lr', X_test_hr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/Y_test_lr', Y_test_hr)\n",
        "  np.save('/content/drive/My Drive/Text to Image/utility_data/test_embeddings', test_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}